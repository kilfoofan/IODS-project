---
title: "Chapter 4"
author: "Arto Vesterbacka"
date: "25 11 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
### Clustering and classification

Here we look at a dataset from Boston on crime.


```{r libraries and data}
library(MASS)
library(dplyr)
library(ggplot2)
library(corrplot)
data("Boston")
```

```{r structure and summary, convert to dataframe}
str(Boston)
summary(Boston)
head(Boston)
boston_scaled <- scale(Boston)
boston_scaled <- as.data.frame(boston_scaled)
```

### The dataset

The dataset, Bostin includes 506 observations wiht 14 variables of numerical crime statistics. The dataset consists of crimes per capita, proportions of zoning, non-retail businesses, owner occupied houses built before 1940, and number of African-American individuals. Also includes the median value of houses in k$ and full tax value in 10k$, NO-emissions, status of population, mean distance from centers, pupils per teacher, nr of rooms per dwelling, accessability index and a dummy variable for Charles River.

### EDA

Exploratory data analysis of the data.

```{r eda}
pairs(boston_scaled)
# calculate the correlation matrix and round it
cor_matrix<-cor(boston_scaled) %>% round(2)

# print the correlation matrix
cor_matrix

# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.6)

# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low","med_high","high"))
# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
```

### Dividing the data to a training set and test set

```{r divide data}
# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]
# save the correct classes from test data
correct_classes <- test$crime
# remove the crime variable from test data
test <- dplyr::select(test, -crime)
```

### Linear discriminant analysis

```{r LDA}
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)
```

### Testing the model

```{r testing}
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
```

### Accuracy of the model
The model seems fairly accurate in predicting the correct classes, having only slight problems when looking at med_high and low classes by putting them into med_low class.

### K-means clustering

```{r kmeans}
data('Boston')
boston_scaled <- scale(Boston)
dist_eu <- dist(boston_scaled)
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

km <-kmeans(boston_scaled, centers = 7)

# plot the Boston dataset with clusters
pairs(boston_scaled, col = km$cluster)


```